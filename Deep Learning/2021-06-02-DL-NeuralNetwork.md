## Neural Network
- 사람의 뉴런을 본따서 만든 딥러닝 기법 

![NN1](./img/NN1.jpg)

그림을 보게 되면 input인 x가 들어오면 입력값에 대한 직접적인 해석이 가장 먼저 이뤄진다. 이때, 해당 뉴런 고유의 해석 방식이 존재하는데 자신만의 신념에 따른 중요도를 직접 부여하게 된다. 즉, w값을 input에 곱해준다.
그 후에는 세포 바디 부분에서 입력값에 대한 동일한 해석을 하게 된다. 이 부분은 코드에서는 static한 상수를 주는 부분이라고 생각하면 된다. 즉, b값을 더해주는 부분이다.
Axon부분에서는 특정치 이상 되는 것만을 출력해주는 부분인데 이 부분에서 향후 나올 activation function이 사용된다.

- Activation Function

![NN2](./img/NN2.jpg)

이 그림이 앞서 언급했던 활성화 함수이다. 해당 그림은 x축에 입력 데이터를 넣고, y축에 결과값 데이터를 넣어서 특정 이상의 값만을 받아들이게 한다. 이 그림에서는 입력값이 12.5가 되어야 이동이 된다고 볼 수 있다.

- Activation Function의 종류

![NN3](./img/NN3.jpg)

대표적으로 활성화 함수에는 Sigmoid 함수가 있는데 성능이 더 좋은 ReLU 함수가 더 많이 쓰인다. 오른쪽 함수들은 난이도가 어려운 모델에서 사용된다고 한다.
**ReLU**는 값이 0보다 작으면 0으로 수렴하게 되고 0 이상이면 무한대의 값을 가지게 된다.

- 활성화 함수를 쓰는 이유?

1. NeuralNet은 인간의 신경망을 본떠서 나온 학습법이여서 인강의 반응과 유사하게 만들어졌다. 즉, 신경전달 자극값이 미세하면 무시하고, 신경전달 자극값이 크면 출력을 위해서 전달되는 형태이다.

2. 수학적 측면에서 활성화 함수가 필요하다.
    - 학습을 아무리 깊이 해도 wx + b 형태에서는 선형이기 때문에 하나의 block과 결과적으로 같아진다. 따라서 독립적으로 값을 유지, 풍부한 값을 출력하기 위해서 활성화 함수로 비선형을 표현할 수 있다.